# 학습 기본 설정
batch_size: 32
num_workers: 4
max_epochs: 10
learning_rate: 0.001

# 데이터로더 설정
dataloader:
  shuffle: true
  drop_last: false
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

# 옵티마이저 설정
optimizer:
  name: "adamw"
  weight_decay: 0.01
  momentum: 0.9  # SGD 사용시

# 스케줄러 설정
scheduler:
  name: "cosine"  # cosine, step, linear
  warmup_epochs: 5
  min_lr: 1e-6

# 메모리 관리
memory_management:
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 32
  persistent_workers: true
  prefetch_factor: 2

# 로깅 설정
logging:
  step_interval: 100  # 몇 스텝마다 로깅할지
  save_graph: true
  metrics:
    - loss
    - learning_rate
    - accuracy
    - f1_score

# 체크포인트 설정
checkpoint:
  dirpath: "checkpoints"
  filename: "model-{epoch:02d}-{val_loss:.2f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  save_last: true

# Early stopping 설정
early_stopping:
  enabled: true
  monitor: "val_loss"
  mode: "min"
  patience: 10
  min_delta: 0.001
